[[Notes/F23/CMPSC 190I/Lecture 1|Lecture 1]]
- LLM's
- Diffusion Model
- Dangers of AI

[[Notes/F23/CMPSC 190I/Lecture 2|Lecture 2]]
- Python Review Notebook
- Numpy
- Matplot

[[Notes/F23/CMPSC 190I/Lecture 3|Lecture 3]]
- Linear Regression
- Ridge Regression

[[Notes/F23/CMPSC 190I/Lecture 4|Lecture 4]]
- Gradient Descent
- Finding the best weights
- How to roll down
- Stochastic Gradient Descent

[[Notes/F23/CMPSC 190I/Lecture 5|Lecture 5]]
- SGD
- Sigmoid
- Logistic Loss Function
- Pseudocode for SGD

[[Notes/F23/CMPSC 190I/Lecture 6|Lecture 6]]
- Multi-Class Classification
- Probabilistic Interpretation
- Maximizing the Data Likelihood
- Tailoring
- Softmax Function
- Training Objective for Multi-Class Classification
- SGD with Cross Entropy Loss

[[Notes/F23/CMPSC 190I/Lecture 7|Lecture 7]]
- Linear Classifier
- Multi-Layer Neural Networks

[[Notes/F23/CMPSC 190I/Lecture 8|Lecture 8]]
- 10 Class Classification Deep Review

[[Notes/F23/CMPSC 190I/Lecture 9|Lecture 9]]
- Simple Nonlinear Prediction
- Multi-layer Neural Network
- Nonlinearity
- XOR Problem

[[Notes/F23/CMPSC 190I/Lecture 10|Lecture 10]]
- Training Neural Networks
- How to find Hyperparameters

[[Notes/F23/CMPSC 190I/Lecture 11|Lecture 11]]
- Multi-Layer Neural Networks
- Back Propagation

[[Notes/F23/CMPSC 190I/Lecture 12|Lecture 12]]
- Backpropagation
- Gates
- Vector Derivatives

[[Notes/F23/CMPSC 190I/Lecture 13|Lecture 13]]
- MISSING

[[Notes/F23/CMPSC 190I/Lecture 14|Lecture 14]]
- MISSING

[[Notes/F23/CMPSC 190I/Lecture 15|Lecture 15]]
- CNN (Convolutional Neural Network)
- 1x1 Convolution
- ResNet

[[Notes/F23/CMPSC 190I/Lecture 16|Lecture 16]]
- MISSING

[[Notes/F23/CMPSC 190I/Lecture 17|Lecture 17]]
- Language Modeling
- Improved RNN Architecture
- Sequence to Sequence Architecture
- Transformer
- Scaled Dot-Product Attention

[[Notes/F23/CMPSC 190I/Lecture 18|Lecture 18]]
- Self-Attention Layer
- Permuting
- Positional Encoding
- Masked Self-Attention Layer
- Transformer Block